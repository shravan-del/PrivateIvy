# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KJTztppfl1nLtxhWxaddWpEoUqkPoQRk
"""

from huggingface_hub import InferenceClient
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import os
import time
import feedparser
# Initialize Hugging Face client for chat completions
# client = InferenceClient("HuggingFaceH4/zephyr-7b-alpha")
# client=InferenceClient('mistralai/Mistral-7B-v0.1')
from openai import OpenAI
import sys
from fastapi import FastAPI
from pydantic import BaseModel


app = FastAPI()


# Set up OpenAI API key - Three different methods:

# Method 1: Set API key directly in the client initialization (not recommended for production)
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))


def update_articles_file():
    feed_url = f"https://medium.com/feed/@sentivity.ai"
    feed = feedparser.parse(feed_url)
    
    if not feed.entries:
        print("No articles found or couldn't fetch feed")
        return False
    
    latest = feed.entries[0]
    new_article = f"{latest.published.split('T')[0]} - {latest.title} - {latest.link}\n"
    articles_path = "articles.txt"
    
    existing_content = ""
    if os.path.exists(articles_path):
        with open(articles_path, "r", encoding="utf-8") as f:
            existing_content = f.read()
        if new_article.strip() in existing_content:
            print("Latest article already in file")
            return False
    
    with open(articles_path, "w", encoding="utf-8") as f:
        f.write(new_article + existing_content)
    print(f"Added new article: {latest.title}")
    return True



#while True:
#   update_articles_file()
#    time.sleep(86400) 



    

# Load base context (Ivy's guidelines and product info)
base_content_path = "base_content.txt"
try:
    with open(base_content_path, "r", encoding="utf-8") as f:
        base_content = f.read()
    print("Successfully loaded base_content.txt")
except FileNotFoundError:
    base_content = "No base content found."
    print("Warning: base_content.txt not found.")

# Load article content (previous Sentivity.ai articles)
articles_file_path = "articles.txt"
try:
    with open(articles_file_path, "r", encoding="utf-8") as f:
        articles_content = f.read()
    print("Successfully loaded articles.txt")
except FileNotFoundError:
    articles_content = "No article content found."
    print("Warning: articles.txt not found.")

# Build TF-IDF vectorizer for retrieval
article_chunks = [chunk.strip() for chunk in articles_content.split("\n\n") if chunk.strip()]
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(article_chunks)
print("TF-IDF matrix shape:", tfidf_matrix.shape)

# Function to retrieve relevant chunks based on query
def retrieve_relevant_chunks(query, top_k=3):
    query_vec = vectorizer.transform([query])
    similarities = cosine_similarity(query_vec, tfidf_matrix)
    top_indices = np.argsort(similarities[0])[::-1][:top_k]
    retrieved = [article_chunks[i] for i in top_indices if i < len(article_chunks)]
    return "\n\n".join(retrieved)

class ChatRequest(BaseModel):
    message: str

# âœ… POST /chat endpoint
@app.post("/chat")
async def chat_endpoint(data: ChatRequest):
    message = data.message


# Ivy's strict response enforcement
IVY_GREETING = "ðŸ‘‹ Hello! Iâ€™m Ivy, Sentivity.aiâ€™s official chatbot. How can I help you today?"



def respond(message, history: list[tuple[str, str]]):
    """
    Handles response generation with strict enforcement that Ivy only discusses Sentivity.ai-related topics.
    """

    # # If chat is empty, Ivy greets the user
    # if not history:
    #     history.append(("", IVY_GREETING))  # Pre-fill the chat with Ivy's greeting
    #     yield history  # Yield the updated history to display the greeting

    system_message = "You are a friendly chatbot, but you must only discuss Sentivity.ai products and official insights."
    max_tokens = 512
    temperature = 0.7
    top_p = 0.95

    # Retrieve relevant chunks from article content based on user query
    retrieved_context = retrieve_relevant_chunks(message)
    retrieval_text = "\n\nRetrieved Articles Content:\n" + retrieved_context

    # Append strict system rules to enforce Sentivity.ai-only discussion
    full_system_message = (
        "You are Ivy, Sentivity.aiâ€™s official chatbot. "
        "WHEN ASKED ABOUT NEWS that means Hive"
        "You must ONLY discuss Sentivity.aiâ€™s products, research, methodologies, and published insights. "
        "You CAN answer questions about sentiment trends, market events, or political topics IF they are reported in Sentivity.ai content. "
        "If the user asks a vague question (e.g., 'anything happen on April 18th?' or 'latest Hive headlines?'), check if Sentivity.ai has a relevant article and return a structured summary. "
        "Do NOT speculate, do NOT refer to outside sources, and do NOT comment on events not covered by Sentivity.ai. "
        "If the user asks about unrelated topics, politely redirect them to Sentivity.aiâ€™s website or Medium page. "
        "Keep your tone helpful, clear, and professional. Use bullet points or numbered lists for headline summaries when appropriate.\n\n"
        + base_content
        + "\n\nRetrieved Articles Content:\n"
        + retrieved_context
        + "\n\n"
        + system_message
    )





    # Construct message history
    messages = [{"role": "system", "content": full_system_message}]
    for user_msg, assistant_msg in history:
        if user_msg:
            messages.append({"role": "user", "content": user_msg})
        if assistant_msg:
            messages.append({"role": "assistant", "content": assistant_msg})

    messages.append({"role": "user", "content": message})

    response = ""

    # # Call the chat completion API (streaming enabled)
    # for api_message in client.chat_completion(
    #     messages,
    #     max_tokens=max_tokens,
    #     stream=True,
    #     temperature=temperature,
    #     top_p=top_p,
    # ):
    #     token = api_message.choices[0].delta.content
    #     response += token
    #     yield response

    try:
        # Call the OpenAI API (streaming enabled)
  # Will use OPENAI_API_KEY environment variable

        stream = client.chat.completions.create(
            model="gpt-4o",  # or "gpt-4" - free version for some reason
            messages=messages,
            max_tokens=max_tokens,
            stream=True,
            temperature=temperature,
            top_p=top_p,
        )

        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                token = chunk.choices[0].delta.content
                response += token
                yield response

    except Exception as e:
        yield f"âš ï¸ An error occurred: {str(e)}"



if __name__ == "__main__":
    port = int(os.environ.get("PORT", 7860))  # Render sets $PORT
    demo.queue().launch(server_name="0.0.0.0", server_port=port)
