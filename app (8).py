# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KJTztppfl1nLtxhWxaddWpEoUqkPoQRk
"""

import gradio as gr
from huggingface_hub import InferenceClient
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import os
# Initialize Hugging Face client for chat completions
# client = InferenceClient("HuggingFaceH4/zephyr-7b-alpha")
# client=InferenceClient('mistralai/Mistral-7B-v0.1')
from openai import OpenAI

# Set up OpenAI API key - Three different methods:

# Method 1: Set API key directly in the client initialization (not recommended for production)
client = OpenAI(api_key=os.getenv('open_api'))

# Load base context (Ivy's guidelines and product info)
base_content_path = "base_content.txt"
try:
    with open(base_content_path, "r", encoding="utf-8") as f:
        base_content = f.read()
    print("Successfully loaded base_content.txt")
except FileNotFoundError:
    base_content = "No base content found."
    print("Warning: base_content.txt not found.")

# Load article content (previous Sentivity.ai articles)
articles_file_path = "articles.txt"
try:
    with open(articles_file_path, "r", encoding="utf-8") as f:
        articles_content = f.read()
    print("Successfully loaded articles.txt")
except FileNotFoundError:
    articles_content = "No article content found."
    print("Warning: articles.txt not found.")

# Build TF-IDF vectorizer for retrieval
article_chunks = [chunk.strip() for chunk in articles_content.split("\n\n") if chunk.strip()]
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(article_chunks)
print("TF-IDF matrix shape:", tfidf_matrix.shape)

# Function to retrieve relevant chunks based on query
def retrieve_relevant_chunks(query, top_k=3):
    query_vec = vectorizer.transform([query])
    similarities = cosine_similarity(query_vec, tfidf_matrix)
    top_indices = np.argsort(similarities[0])[::-1][:top_k]
    retrieved = [article_chunks[i] for i in top_indices if i < len(article_chunks)]
    return "\n\n".join(retrieved)

# Ivy's strict response enforcement
IVY_GREETING = "ðŸ‘‹ Hello! Iâ€™m Ivy, Sentivity.aiâ€™s official chatbot. How can I help you today?"



def respond(message, history: list[tuple[str, str]]):
    """
    Handles response generation with strict enforcement that Ivy only discusses Sentivity.ai-related topics.
    """

    # # If chat is empty, Ivy greets the user
    # if not history:
    #     history.append(("", IVY_GREETING))  # Pre-fill the chat with Ivy's greeting
    #     yield history  # Yield the updated history to display the greeting

    system_message = "You are a friendly chatbot, but you must only discuss Sentivity.ai products and official insights."
    max_tokens = 512
    temperature = 0.7
    top_p = 0.95

    # Retrieve relevant chunks from article content based on user query
    retrieved_context = retrieve_relevant_chunks(message)
    retrieval_text = "\n\nRetrieved Articles Content:\n" + retrieved_context

    # Append strict system rules to enforce Sentivity.ai-only discussion
    full_system_message = (
        "You are Ivy, Sentivity.aiâ€™s official chatbot. You must ONLY discuss Sentivity.ai products, research, and methodologies. "
        "You can provide info on companies, NEWS headlines, and snetiment IF it is made my sentivity.ai "
        "other wise do not comment on external stuff"
        "Redirect off-topic questions to Sentivity.aiâ€™s website.\n\n"
        + base_content
        + retrieval_text
        + "\n\n" + system_message
    )

    # Construct message history
    messages = [{"role": "system", "content": full_system_message}]
    for user_msg, assistant_msg in history:
        if user_msg:
            messages.append({"role": "user", "content": user_msg})
        if assistant_msg:
            messages.append({"role": "assistant", "content": assistant_msg})

    messages.append({"role": "user", "content": message})

    response = ""

    # # Call the chat completion API (streaming enabled)
    # for api_message in client.chat_completion(
    #     messages,
    #     max_tokens=max_tokens,
    #     stream=True,
    #     temperature=temperature,
    #     top_p=top_p,
    # ):
    #     token = api_message.choices[0].delta.content
    #     response += token
    #     yield response

    try:
        # Call the OpenAI API (streaming enabled)
  # Will use OPENAI_API_KEY environment variable

        stream = client.chat.completions.create(
            model="gpt-4",  # or "gpt-4"
            messages=messages,
            max_tokens=max_tokens,
            stream=True,
            temperature=temperature,
            top_p=top_p,
        )

        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                token = chunk.choices[0].delta.content
                response += token
                yield response

    except Exception as e:
        yield f"âš ï¸ An error occurred: {str(e)}"

# Set up Gradio Chat Interface
demo = gr.ChatInterface(respond)

# Run chatbot
if __name__ == "__main__":
    demo.launch(share=True)